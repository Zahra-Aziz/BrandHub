{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import unicodecsv as ucsv\n",
    "import re \n",
    "from selenium import webdriver\n",
    "import time \n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from os.path  import basename\n",
    "import os\n",
    "import FeatureVector\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "cd = FeatureVector.ColorDescriptor((8, 12, 3))\n",
    "access_token=\"ya29.a0AfH6SMBc2gCWfdkSRZDwIGNS2mOD9sWrPlYTrc1v2WZfO8ZnfCumBqkKYKP8fNUZbUXTrzJOCOTxpL40OsSZS8UAdH4BxCIWHmBmvDjXHf2sTdXrFGI_6sHoq2K-MJtdE1ZCw8qhPNV9vz6WC50vZbUApQanyM1L1NijbvoHWTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrollBrowser(url,productID): #for scrollable websites\n",
    "    browser2 = webdriver.Chrome()\n",
    "    browser2.get(url)\n",
    "    browser2.page_source\n",
    "    delay = 50 # seconds\n",
    "\n",
    "    #open browser, scroll till end and then start scraping\n",
    "    try:\n",
    "        browser2.find_element_by_class_name(\"infinite-scrolling\").click()\n",
    "        browser2.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        #browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        myElem = WebDriverWait(browser2, delay).until(EC.presence_of_element_located((By.ID, productID)))\n",
    "        print (\"Page is ready!\")\n",
    "    except TimeoutException:\n",
    "        print (\"Loading took too much time!\")\n",
    "    return browser\n",
    "\n",
    "def InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors):\n",
    "    document = {\"PId\":id_name,\n",
    "                \"PName\":product_name,\n",
    "                \"PPrice\":price,\n",
    "                \"OnSale\":sale,\n",
    "                \"SPrice\":sale_price,\n",
    "                \"Stock\":stock,\n",
    "                \"Color\":Color,\n",
    "                \"Link\":product_link,\n",
    "                \"Fabric\":Fabric,\n",
    "                \"Description\":Description,\n",
    "                \"Type\":Type,\n",
    "               \"ImageName\": images_arr,\n",
    "               \"featureVectors\":str(feature_vectors)}\n",
    "    id1=db_collection.insert_one(document) #insert document in mongodb\n",
    "    return id1\n",
    "    \n",
    "def StoreonGoogleDrive(name,folderID,path):\n",
    "    headers = {\"Authorization\": \"Bearer \"+ access_token}\n",
    "    para = {\n",
    "        \"name\": name, #name of image in PC\n",
    "        \"parents\" : [folderID] #specify which folder it should go to\n",
    "    }\n",
    "    files = {\n",
    "        'data': ('metadata', json.dumps(para), 'application/json; charset=UTF-8'),\n",
    "        'file': open(path, \"rb\")\n",
    "    }\n",
    "    r = requests.post(\n",
    "        \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\",\n",
    "        headers=headers,\n",
    "        files=files\n",
    "    )\n",
    "\n",
    "    jsonobj=json.loads(r.text) #get text from request which contains the path of image in google drive\n",
    "    image_path=\"https://drive.google.com/file/d/\"+jsonobj['id']+\"/view\" #path of image in google drive\n",
    "    return image_path\n",
    "\n",
    "def getDbCon(Brand,Gender):\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://zahra:passmongodb@cluster0.femwg.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client[Brand]\n",
    "    db_collection=db[Gender]\n",
    "    return db_collection\n",
    "                            \n",
    "\n",
    "def CambridgeScrape():\n",
    "    #get browser by giving url and last productid\n",
    "    browser=scrollBrowser(\"https://thecambridgeshop.com/collections/designer-shalwar-kameez\",'product-4939290738774')\n",
    "    \n",
    "    #accessing html tags\n",
    "    container=browser.find_elements_by_class_name('product-collection products-grid row')\n",
    "    image_count=1\n",
    "\n",
    "    divv=browser.find_elements_by_xpath(\"//div[@class='product-image image-swap']\")\n",
    "    product_count=0\n",
    "    \n",
    "    for d in divv:\n",
    "        #empty variabls before every iteration\n",
    "        price=\"\"\n",
    "        Fabric=\"\"\n",
    "        Color=\"\"\n",
    "        sale=\"\"\n",
    "        sale_price=\"\"\n",
    "        Description=\"\"\n",
    "        stock=\"\"\n",
    "\n",
    "\n",
    "        product_link=d.find_element_by_css_selector('a').get_attribute('href') #select all links \n",
    "        print(\"product Count: \"+str(product_count))\n",
    "\n",
    "        print(\"Product Link: \"+product_link)\n",
    "        if (product_count>0):\n",
    "            request = requests.get(product_link) #get request from link\n",
    "            if (request.status_code == 200): #if link exists\n",
    "                html=urlopen(product_link)\n",
    "                soup=BeautifulSoup(html, \"html.parser\") #use beautifulsoup to parse through items\n",
    "                product_name=soup.h1.text.strip() #get product name from heading \n",
    "\n",
    "                price_container=soup.findAll(\"div\",{\"class\":\"prices\"}) #get price of product\n",
    "                price=price_container[0].span.span.text.strip() \n",
    "\n",
    "                sale_container=soup.findAll(\"span\",{\"class\":\"price on-sale\"}) #get to know if product is on sale\n",
    "                if (sale_container):\n",
    "                    sale=True\n",
    "                    sale_price=sale_container[0].span.text.strip()\n",
    "                else:\n",
    "                    sale=False\n",
    "                    sale_price=\"\"\n",
    "\n",
    "                Type=\"Shalwar Kameez\" #specify type of product\n",
    "\n",
    "                description_container=soup.findAll(\"div\",{\"class\":\"short-description\"}) #store description\n",
    "                Description=description_container[0].text.strip()\n",
    "\n",
    "                color_container=soup.findAll(\"div\",{\"class\":\"tooltip\"}) #store color\n",
    "                if (color_container):\n",
    "                    Color=color_container[0].text.strip()\n",
    "\n",
    "                fabric_container=soup.findAll(\"div\",{\"data-option-index\":\"2\"}) #store fabric\n",
    "                if (fabric_container):\n",
    "                    Fabric=fabric_container[0].label.text.strip()\n",
    "                  \n",
    "                #get container of images of product\n",
    "                images_container=soup.findAll(\"div\",{\"class\":\"test product-img-box left-vertical-moreview vertical-moreview\"})\n",
    "               \n",
    "\n",
    "                if (images_container):\n",
    "                    images=images_container[0].div.div #get all images\n",
    "                    images_arr=[]\n",
    "                    feature_vectors=[]\n",
    "\n",
    "\n",
    "                    if len(images_container)!=0:\n",
    "                        for div in images.findAll('div'): #iterate through all images\n",
    "                            image=div.a\n",
    "                            url=image['href']   #gets the image but with extra characters\n",
    "                            split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                            substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "\n",
    "                            response = requests.get(\"https:\"+substring) #access image through requests\n",
    "                            file = open(\"CM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                            file.write(response.content) #write image in our own file. It is downloaded\n",
    "                            file.close()\n",
    "\n",
    "                            id_name=\"CM_\"+str(product_count)+\"_\" #id of product\n",
    "\n",
    "                            #stores files in google drive:\n",
    "                            name=\"CM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "                            path=os.path.normpath('C:/Users/dell/'+name) #access image from PC\n",
    "\n",
    "                            image_path=StoreonGoogleDrive(name,\"1Fu2Y5Dev990ax0Ly3eetHgJyr9Ekkykc\",path) #this function stores the image with that name from path on google drive\n",
    "                           \n",
    "                            #read image, calcualte feature vectors and append in 2D array of feature vectors\n",
    "                            image = cv2.imread(name)\n",
    "                            features = cd.describe(image)\n",
    "                            feature_vectors.append([])\n",
    "                            feature_vectors[image_count-1]=features\n",
    "                        \n",
    "                            images_arr.append(image_path) #append images of product in image array\n",
    "                            image_count=image_count+1\n",
    "\n",
    "                    #store all the variables as key value pairs in mongodb\n",
    "                    db_collection=getDbCon(Brand,Gender)\n",
    "                    id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "                    print(id1)\n",
    "\n",
    "                    image_count=1\n",
    "                print(\"\")\n",
    "        product_count+=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JdotScrape():\n",
    "    i=1 #for product count\n",
    "    page_number=1\n",
    "    product_count=1\n",
    "    image_count=1\n",
    "    condition=True\n",
    "\n",
    "    #make connection with db for Jdot \n",
    "    client = pymongo.MongoClient(\"mongodb+srv://zahra:passmongodb@cluster0.femwg.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client['Jdot']\n",
    "    db_collection=db['Women']\n",
    "\n",
    "    while (condition):\n",
    "        url= 'https://www.junaidjamshed.com/womens/stitched.html?p={}'.format(page_number) #open J. link, the format(page) inserts numbers in the page array into the url\n",
    "        html=urlopen(url)\n",
    "        soup=BeautifulSoup(html, \"html.parser\") #send link and tell format(2nd arg)\n",
    "        if (page_number==1):\n",
    "            title = soup.title\n",
    "            print(title.text,\"\\n\")\n",
    "\n",
    "        containers=soup.findAll(\"li\",{\"class\":\"item product product-item\"})\n",
    "        if (len(containers)==0): #page doesn't have shit meaning page doesn't exist\n",
    "            condition=False\n",
    "            print(\"\\nScraping Finished. Total Pages: \"+str(page_number-1))\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nPAGE#\"+str(page_number)) \n",
    "            container=containers[0]\n",
    "\n",
    "            link_containers=soup.findAll(\"div\",{\"class\":\"product_image\"}) #access links of all products on page\n",
    "\n",
    "            for link in link_containers: #iterate through all links\n",
    "                print(\"Product#\"+str(product_count))\n",
    "\n",
    "                if (product_count>0):\n",
    "                    linkurl=link.a['href'] #access link\n",
    "                    html=urlopen(linkurl) #open link\n",
    "                    soup=BeautifulSoup(html, \"html.parser\") #use Beautiful Soup to parse\n",
    "                    print(\"Product Link: \"+linkurl)\n",
    "                    \n",
    "                    #initialize everything null before iterating\n",
    "                    stock=\"\"\n",
    "                    price=\"\"\n",
    "                    Color=\"\"\n",
    "                    Fabric=\"\"\n",
    "                    type1=\"\"\n",
    "                    Desc=\"\"\n",
    "                    Deatils=\"\"\n",
    "                    sale=\"\"\n",
    "                    sale_price=\"\"\n",
    "\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"price-box price-final_price\"})\n",
    "                    price=price_container[0].span.span.span.text.strip() #store price\n",
    "\n",
    "\n",
    "                    stock_container=soup.findAll(\"div\",{\"class\":\"stock available\"})\n",
    "                    if len(stock_container)!=0:\n",
    "                        stock=stock_container[0].span.text.strip() #store if product is in stock or not\n",
    "\n",
    "                    \n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"additional-attributes-wrapper table-wrapper\"})\n",
    "                    if len(description_container)!=0:\n",
    "                        color=description_container[0].table.tbody.tr.td.text\n",
    "                        description=description_container[0].table.tbody #access description container\n",
    "\n",
    "                        k=1\n",
    "                        #extract individual components from description and store in relevant variables\n",
    "                        for tr in description.findAll('tr'): \n",
    "                            Info=tr.th.text\n",
    "                            if (Info.startswith('Color')):\n",
    "                                Color = tr.td.text #store color\n",
    "                            elif (Info.startswith('Product Category')):\n",
    "                                type1 = tr.td.text #xtore type\n",
    "                            elif (Info.startswith('Fabric')):\n",
    "                                Fabric = tr.td.text #store fabric\n",
    "\n",
    "\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"product attribute overview\"})\n",
    "                    if len(description_container)!=0:\n",
    "                        description=description_container[0].div.p #store description\n",
    "\n",
    "\n",
    "                        k=1\n",
    "                        #store description,collection, details depending on the html\n",
    "                        for br in description.findAll('br'):\n",
    "                            Details = br.nextSibling\n",
    "                            br.replace_with(\"\")\n",
    "                            k+=1\n",
    "\n",
    "                        Desc=description.text\n",
    "                        if (Desc.startswith('Collection')):\n",
    "                            k=1\n",
    "                        else:\n",
    "                            Desc=\"\"\n",
    "\n",
    "                        if (Details.startswith('Description')):\n",
    "                            k=1\n",
    "                        else:\n",
    "                            Details=\"\"\n",
    "\n",
    "                    images_arr=[] \n",
    "                    feature_vectors=[]\n",
    "                    image_container=soup.findAll('a', {'data-image':re.compile('.jpg')}) #access images\n",
    "                    \n",
    "                    for image in image_container: #iterate through images\n",
    "                        url=image['data-image']   #gets the image but with extra characters\n",
    "                        split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                        substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "                       \n",
    "                         #downloads file:\n",
    "                        response = requests.get(substring)\n",
    "                        file = open(\"JF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                        file.write(response.content) #write image in our file\n",
    "                        file.close()\n",
    "\n",
    "                        id_name=\"JF_\"+str(product_count)+\"_\" \n",
    "\n",
    "                        #stores files in google drive:\n",
    "                        name=\"JF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "                        #path=r\"C:\\Users\\dell\\n\" + name\n",
    "                        path=os.path.normpath('C:/Users/dell/'+name) \n",
    "\n",
    "                       \n",
    "                        image_path=StoreonGoogleDrive(name,\"14qncJNgoTFvVxjsmXF2bpCsop6XqQpy8\",path) #this function stores the image with that name from path on google drive\n",
    "                        #print(image_path)\n",
    "\n",
    "                        image = cv2.imread(name)\n",
    "                        features = cd.describe(image)\n",
    "                        feature_vectors.append([])\n",
    "                        feature_vectors[image_count-1]=features\n",
    "\n",
    "\n",
    "\n",
    "                        images_arr.append(image_path)\n",
    "                        image_count=image_count+1\n",
    "\n",
    "\n",
    "\n",
    "                    db_collection=getDbCon(Brand,Gender) #specify brand and gender to get db\n",
    "                    #store data in db\n",
    "                    id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "                    print(id1)\n",
    "                    image_count=1\n",
    "                product_count+=1\n",
    "                print(\"\\n\")\n",
    "\n",
    "            page_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KhaadiScrape():\n",
    "    i=1 #for product count\n",
    "    image_count=1\n",
    "    page_number=1\n",
    "    product_count=1\n",
    "    count=1\n",
    "    condition=True\n",
    "\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://zahra:passmongodb@cluster0.femwg.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client['khaadi']\n",
    "    db_collection=db['Women']\n",
    "   \n",
    "    while (condition):\n",
    "        url= 'https://pk.khaadi.com/ready-to-wear.html?p={}'.format(page_number) #open khaadi link, the format(page) inserts numbers in the page array into the url\n",
    "        html=urlopen(url)\n",
    "\n",
    "        soup=BeautifulSoup(html, \"html.parser\") #send link and tell format(2nd arg)\n",
    "\n",
    "        if (page_number==1):\n",
    "            title = soup.title\n",
    "            print(title.text,\"\\n\")\n",
    "\n",
    "        containers= soup.findAll(\"div\",{\"class\":\"products wrapper grid products-grid\"}) #grabs each product\n",
    "\n",
    "        if (len(containers)==0): #page doesn't have shit meaning page doesn't exist\n",
    "            condition=False\n",
    "            print(\"\\nScraping Finished. Total Pages: \"+str(page_number-1))\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nPAGE#\"+str(page_number)) \n",
    "            container=containers[0]\n",
    "\n",
    "            link_containers=soup.findAll(\"div\",{\"class\":\"product-top\"})\n",
    "\n",
    "            type1=\"\"\n",
    "            id_name=\"\"\n",
    "            price=\"\"\n",
    "            stock=\"\"\n",
    "            sale=\"\"\n",
    "            sale_price=\"\"\n",
    "            Color=\"\"\n",
    "            material=\"\"\n",
    "            description=\"\"\n",
    "\n",
    "            for link in link_containers:\n",
    "                if (product_count>0):\n",
    "                    linkurl=link.a['href']\n",
    "                    html=urlopen(linkurl)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "                    print(\"Product Number: \"+str(product_count))\n",
    "                    print(\"Proudct Link: \"+linkurl)\n",
    "                    print(\"Proudct Name: \"+soup.title.text)\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"price-box price-final_price\"})\n",
    "                    price=price_container[0].span.span.span.next_element.next_element.next_element.next_element.text.strip()\n",
    "           \n",
    "\n",
    "                    stock_container=soup.findAll(\"div\",{\"class\":\"product-info-stock-sku\"})\n",
    "                    if len(stock_container)!=0:\n",
    "                        stock=stock_container[0].div.span.text.strip()\n",
    "\n",
    "                    material_container=soup.findAll(\"div\",{\"class\":\"product-attribute-material\"})\n",
    "                    if len(material_container)!=0:\n",
    "                        material=material_container[0].span.next_element.next_element.text\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"product-sub-infomation\"})\n",
    "                    if len(description_container)!=0:\n",
    "                        description=description_container[0].text.strip()\n",
    "\n",
    "                    image_container=soup.findAll('div', {'data-zoom':re.compile('.jpg')})\n",
    "\n",
    "                    type1=soup.title.text\n",
    "\n",
    "                    images_arr=[]\n",
    "                    feature_vectors=[]\n",
    "\n",
    "                    for image in image_container:\n",
    "                        url=image['data-zoom']   #gets the image but with extra characters\n",
    "                        split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                        substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "                        #print(\"Image: \"+substring)\n",
    "\n",
    "                        #downloads file:\n",
    "                        response = requests.get(substring)\n",
    "                        file = open(\"KF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                        file.write(response.content)\n",
    "                        file.close()\n",
    "\n",
    "                        id_name=\"KF_\"+str(product_count)+\"_\"\n",
    "\n",
    "                        #stores files in google drive:\n",
    "                        name=\"KF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "                        #path=r\"C:\\Users\\dell\\n\" + name\n",
    "                        path=os.path.normpath('C:/Users/dell/'+name) \n",
    "\n",
    "                        \n",
    "                        image_path=StoreonGoogleDrive(name,\"10pDBHseIITjTloZXL4LxkDr4BsGZcF2e\",path) #this function stores the image with that name from path on google drive\n",
    "\n",
    "\n",
    "                        image = cv2.imread(name)\n",
    "                        features = cd.describe(image)\n",
    "                        feature_vectors.append([])\n",
    "                        feature_vectors[image_count-1]=features\n",
    "\n",
    "                        images_arr.append(image_path)\n",
    "                        image_count=image_count+1\n",
    "                        \n",
    "                    \n",
    "                    db_collection=getDbCon(Brand,Gender) #specify brand and gender to get db\n",
    "                    #store data in db\n",
    "                    id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "                    print(id1)\n",
    "\n",
    "                image_count=1\n",
    "                product_count+=1\n",
    "                count+=1\n",
    "                print(\"\")\n",
    "                \n",
    "            page_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutfitterScrape():\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://zahra:passmongodb@cluster0.femwg.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client['Outfitters']\n",
    "    db_collection=db['Men']\n",
    "        \n",
    "    #get browser by giving url and last productid\n",
    "    browser1=scrollBrowser(\"https://outfitters.com.pk/collections/new-men\",'4479509332048')\n",
    "\n",
    "    divv=browser1.find_elements_by_xpath(\"//div[@class='product-bottom']\")\n",
    "    print(len(divv))\n",
    "    product_count=1\n",
    "    image_count=1\n",
    "    for d in divv:\n",
    "        if (d.text):\n",
    "            product_link=d.find_element_by_css_selector('a').get_attribute('href')\n",
    "            print(\"product Count: \"+str(product_count))\n",
    "            print(\"Product Link: \"+product_link)\n",
    "            if (product_count>331):\n",
    "\n",
    "                price=\"\"\n",
    "                Fabric=\"\"\n",
    "                Color=\"\"\n",
    "                sale=\"\"\n",
    "                stock=\"\"\n",
    "                sale_price=\"\"\n",
    "                Description=\"\"\n",
    "                Type=\"\"\n",
    "                images_arr=[]\n",
    "                feature_vectors=[]\n",
    "\n",
    "                request = requests.get(product_link)\n",
    "                if (request.status_code == 200):\n",
    "                    html=urlopen(product_link)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                    product_name=soup.h1.text.strip()\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"prices\"})\n",
    "                    price=price_container[0].span.span.text.strip()\n",
    "\n",
    "                    sale_container=soup.findAll(\"span\",{\"class\":\"price on-sale\"})\n",
    "                    if (sale_container):\n",
    "                        sale=True\n",
    "                        sale_price=sale_container[0].span.text.strip()\n",
    "                    else:\n",
    "                        sale=False\n",
    "                        sale_price=\"\"\n",
    "\n",
    "                    color_container=soup.findAll(\"div\",{\"class\":\"Rcolor-header\"})\n",
    "                    if (color_container):\n",
    "                        Color=color_container[0].p.span.text.strip()\n",
    "     \n",
    "\n",
    "                    fabric_container=soup.findAll(\"ul\",{\"data-mce-fragment\":\"1\"})\n",
    "                    if (fabric_container):\n",
    "                        if (fabric_container[0].li.text.strip().find(\"%\")==-1): #because fabric is given in percentages\n",
    "                            Fabric=\"\"\n",
    "                        else:\n",
    "                            Fabric=fabric_container[0].li.text.strip()\n",
    "\n",
    "                    images_container=soup.findAll(\"div\",{\"class\":\"col-md-6 product-photos\"})\n",
    "                    print(len(images_container))\n",
    "\n",
    "                    if (images_container):\n",
    "                        images=images_container[0].div.div.div\n",
    "                        #print(images)\n",
    "                        images_arr=[]\n",
    "                        feature_vectors=[]\n",
    "\n",
    "                    if len(images_container)!=0:\n",
    "                        for div in images.findAll('div'):\n",
    "                            #image_container=images_container[0].findAll('a', {'href':re.compile('.jpg')})\n",
    "                            image=div.a\n",
    "                           # print(image)\n",
    "                            #for image in image_container:\n",
    "                            url=image['href']   #gets the image but with extra characters\n",
    "                            split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                            substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "        #                     print(substring)\n",
    "\n",
    "                            response = requests.get(\"https:\"+substring)\n",
    "                            file = open(\"OM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                            file.write(response.content)\n",
    "                            file.close()\n",
    "\n",
    "                            id_name=\"OM_\"+str(product_count)+\"_\"\n",
    "\n",
    "                            #stores files in google drive:\n",
    "                            name=\"OM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "                            #path=r\"C:\\Users\\dell\\n\" + name\n",
    "                            path=os.path.normpath('C:/Users/dell/'+name) \n",
    "                            #print(path)\n",
    "\n",
    "                            \n",
    "                            image_path=StoreonGoogleDrive(name,\"1b1ICrKiPTTyVqC4EI37BKKAh2cn8ypt4\",path) #this function stores the image with that name from path on google drive\n",
    "                            #print(image_path)\n",
    "\n",
    "                            image = cv2.imread(name)\n",
    "                            features = cd.describe(image)\n",
    "                            feature_vectors.append([])\n",
    "                            feature_vectors[image_count-1]=features\n",
    "\n",
    "                            images_arr.append(image_path)\n",
    "                            image_count=image_count+1\n",
    "\n",
    "\n",
    "                    \n",
    "                    db_collection=getDbCon(Brand,Gender) #specify brand and gender to get db\n",
    "                    #store data in db\n",
    "                    id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "                    print(id1)\n",
    "\n",
    "                    image_count=1\n",
    "\n",
    "        \n",
    "        \n",
    "        product_count+=1\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LimelightScrape():\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://zahra:passmongodb@cluster0.femwg.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client['Limelight']\n",
    "    db_collection=db['Women']\n",
    "    \n",
    "   \n",
    "    browser=scrollBrowser(\"https://www.limelight.pk/collections/trousers-1\",'prod-1339407499352')\n",
    "        \n",
    "    \n",
    "    condition=True\n",
    "    container=browser.find_elements_by_class_name('product-grid-3')\n",
    "\n",
    "\n",
    "    image_count=1\n",
    "    divv=browser.find_elements_by_xpath(\"//div[@class='ci alche-ci']\")\n",
    "    print(len(divv))\n",
    "    product_count=0\n",
    "    for d in divv:\n",
    "        if (d.text):\n",
    "\n",
    "            product_name=d.text\n",
    "            product_link=d.find_element_by_css_selector('a').get_attribute('href')\n",
    "            print(\"product Count: \"+str(product_count))\n",
    "            print(\"Product Link: \"+product_link)\n",
    "            product_count+=1\n",
    "            if (product_count>471):\n",
    "\n",
    "                price=\"\"\n",
    "                Fabric=\"\"\n",
    "                Color=\"\"\n",
    "                sale=\"\"\n",
    "                sale_price=\"\"\n",
    "                Description=\"\"\n",
    "                Type=\"\"\n",
    "                stock=\"\"\n",
    "\n",
    "                request = requests.get(product_link)\n",
    "                if (request.status_code == 200):\n",
    "                    html=urlopen(product_link)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "                    product_name=soup.h1.text.strip()\n",
    "\n",
    "\n",
    "                    sale_container=soup.findAll(\"span\",{\"class\":\"was\"})\n",
    "                    if (sale_container):\n",
    "                        sale=True\n",
    "                        price=sale_container[0].span.text.strip()\n",
    "                        price_container=soup.findAll(\"span\",{\"class\":\"product-price\"})\n",
    "                        sale_price=price_container[0].span.text.strip()\n",
    "                    else:\n",
    "                        sale=False\n",
    "                        sale_price=\"\"\n",
    "                        price_container=soup.findAll(\"span\",{\"class\":\"product-price\"})\n",
    "                        if (price_container[0]!=None):\n",
    "                            price=price_container[0].span.text.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"product-discription\"})\n",
    "                    #print(len(description_container))\n",
    "                    if len(description_container)!=0:\n",
    "                        #color=description_container[0].table.tbody.tr.td.ul.li\n",
    "                        description=description_container[0].table.tbody.tr.td.ul\n",
    "\n",
    "\n",
    "                        k=1\n",
    "                        Desc=\"\"\n",
    "                        Color=\"\"\n",
    "                        Fabric=\"\"\n",
    "                        for li in description.findAll('li'):\n",
    "                            Info=li.text.strip()\n",
    "\n",
    "\n",
    "                            if (Info.startswith('Color')):\n",
    "                                Color=Info\n",
    "                            if (Info.startswith('Fabric')):\n",
    "                                #print(\"First IF\")\n",
    "                                Fabric=Info\n",
    "                            elif (Info.startswith('Model')):\n",
    "                                k+=1\n",
    "                                #print(\"SEcomd if\")\n",
    "                            else:\n",
    "                                Desc+=Info\n",
    "                                Desc+='\\n'\n",
    "\n",
    "                            k+=1\n",
    "\n",
    "                        Type=\"Trouser\"\n",
    "\n",
    "\n",
    "                        images_container=soup.findAll(\"div\",{\"class\":\"thumbnail-slider1\"})\n",
    "                        if (len(images_container)==0):\n",
    "                            images_container=soup.findAll(\"div\",{\"class\":\"desktop-12 thumbnail-gallery\"})\n",
    "\n",
    "                        images_arr=[]\n",
    "                        feature_vectors=[]\n",
    "\n",
    "                        if len(images_container)!=0:\n",
    "                            for div in images_container[0].findAll('div'):\n",
    "                                #image_container=images_container[0].findAll('a', {'href':re.compile('.jpg')})\n",
    "                                image=div.a\n",
    "                                #for image in image_container:\n",
    "                                url=image['href']   #gets the image but with extra characters\n",
    "                                split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                                substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "    #                             print(substring)\n",
    "\n",
    "                                response = requests.get(\"https:\"+substring)\n",
    "                                file = open(\"LW_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                                file.write(response.content)\n",
    "                                file.close()\n",
    "\n",
    "                                id_name=\"LW_\"+str(product_count)+\"_\"\n",
    "\n",
    "                                #stores files in google drive:\n",
    "                                name=\"LW_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "                                #path=r\"C:\\Users\\dell\\n\" + name\n",
    "                                path=os.path.normpath('C:/Users/dell/'+name) \n",
    "                                #print(path)\n",
    "\n",
    "                              \n",
    "                                image_path=StoreonGoogleDrive(name,\"13ivBACuP65K6JR1AILYBO2sPQP5-WyYw\",path) #this function stores the image with that name from path on google drive\n",
    "\n",
    "                                image = cv2.imread(name)\n",
    "                                features = cd.describe(image)\n",
    "                                feature_vectors.append([])\n",
    "                                feature_vectors[image_count-1]=features\n",
    "\n",
    "\n",
    "\n",
    "                                images_arr.append(image_path)\n",
    "                                image_count=image_count+1\n",
    "\n",
    "\n",
    "                            \n",
    "                            db_collection=getDbCon(Brand,Gender) #specify brand and gender to get db\n",
    "                            #store data in db\n",
    "                            id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "                            print(id1)\n",
    "\n",
    "                            image_count=1\n",
    "                        print(\"\")\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SapphireScrape():\n",
    "    i=1 #for product count\n",
    "    product_count=1\n",
    "    saved=222\n",
    "    image_count=1\n",
    "    page_number=1\n",
    "    condition=True\n",
    "\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://zahra:passmongodb@cluster0.femwg.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client['sapphire']\n",
    "    db_collection=db['Women']\n",
    "    cd = FeatureVector.ColorDescriptor((8, 12, 3))\n",
    "\n",
    "    while (condition):\n",
    "\n",
    "\n",
    "\n",
    "        url= 'https://pk.sapphireonline.pk/collections/ready-to-wear?p={}'.format(page_number) #open khaadi link, the format(page) inserts numbers in the page array into the url\n",
    "        html=urlopen(url)\n",
    "\n",
    "        soup=BeautifulSoup(html, \"html.parser\") #send link and tell format(2nd arg)\n",
    "\n",
    "        if (page_number==1):\n",
    "            title = soup.title\n",
    "            print(title.text,\"\\n\")\n",
    "\n",
    "        containers= soup.findAll(\"div\",{\"class\":\"product-collection products-grid row-bt\"}) #grabs each product\n",
    "\n",
    "        if (len(containers)==0): #page doesn't have shit meaning page doesn't exist\n",
    "            condition=False\n",
    "            print(\"\\nScraping Finished. Total Pages: \"+str(page_number-1))\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nPAGE#\"+str(page_number)) \n",
    "            container=containers[0]\n",
    "\n",
    "            link_containers=soup.findAll(\"div\",{\"class\":\"grid-item col-6 col-md-4 col-xl-3 four-columns\"})\n",
    "            print(\"linkk: \"+str(len(link_containers)))\n",
    "\n",
    "            for link in link_containers:\n",
    "                if (product_count>510 ):\n",
    "                    linkurl=\"https://pk.sapphireonline.pk/\" +link.a['href']\n",
    "                    print(\"Product# \"+str(product_count))\n",
    "                    print(\"Product Link: \"+linkurl)\n",
    "                    html=urlopen(linkurl)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "                    print(\"Product Name: \"+soup.h2.span.text)\n",
    "\n",
    "                    Fabric=\"\"\n",
    "                    Color=\"\"\n",
    "                    price=\"\"\n",
    "                    stock=\"\"\n",
    "                    type1=\"\"\n",
    "                    sale=\"\"\n",
    "                    sale_price=\"\"\n",
    "                    images_arr=[]\n",
    "                    feature_vectors=[]\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"prices\"})\n",
    "                    price=price_container[0].span.text.strip()\n",
    "                   # print(\"Price: \"+price)\n",
    "\n",
    "                    stock_container=soup.findAll(\"div\",{\"class\":\"product-inventory\"})\n",
    "                    if len(stock_container)!=0:\n",
    "                        stock=stock_container[0].span.text.strip()\n",
    "                     #   print(\"Stock: \"+stock)\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"short-description\"})\n",
    "                    #print(len(description_container))\n",
    "\n",
    "                    if len(description_container)!=0:\n",
    "                        description=description_container[0].p.text\n",
    "                        print(\"Description: \"+ str(description))\n",
    "\n",
    "\n",
    "                        moredesc=description_container[0].p.next_element.next_element.next_element.next_element.next_element.next_element.next_element\n",
    "                        #print(moredesc)\n",
    "\n",
    "                        if isinstance(description_container[0].p.next_element, NavigableString):\n",
    "                            k=1\n",
    "                        else:\n",
    "                            type1=description_container[0].p.next_element.text.strip()\n",
    "                        #print(\"type: \"+type1)\n",
    "\n",
    "\n",
    "\n",
    "                        if (moredesc!=None):\n",
    "                            if (isinstance(moredesc, NavigableString)):\n",
    "                                k=1\n",
    "                            else:\n",
    "\n",
    "                                for strong in moredesc.findAll('strong'):\n",
    "                                    Info = strong.text.strip()\n",
    "                                    if (Info.startswith('Fabric')):\n",
    "                                        Fabric=strong.next_element.next_element.strip()\n",
    "\n",
    "                                    elif (Info.startswith('Color')):\n",
    "                                        Color=strong.next_element.next_element.strip()\n",
    "\n",
    "                                    else:\n",
    "                                        k=1\n",
    "\n",
    "\n",
    "                    images_arr=[]\n",
    "                    feature_vectors=[]\n",
    "                    images_container=soup.findAll(\"div\",{\"class\":\"MagicToolboxSelectorsContainer no-magic-scroll\"})\n",
    "\n",
    "                    if len(images_container)!=0:\n",
    "                        for image in images_container[0].findAll('a'):\n",
    "                            #image_container=images_container[0].findAll('a', {'href':re.compile('.jpg')})\n",
    "                            #image=div.a\n",
    "                            #for image in image_container:\n",
    "                            url=image['href']   #gets the image but with extra characters\n",
    "                            split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                            substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "                           # print(substring)\n",
    "\n",
    "                              #downloads file:\n",
    "                            response = requests.get(\"https:\"+substring)\n",
    "                            file = open(\"SF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                            file.write(response.content)\n",
    "                            file.close()\n",
    "\n",
    "                            id_name=\"SF_\"+str(product_count)+\"_\"\n",
    "\n",
    "                            #stores files in google drive:\n",
    "                            name=\"SF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "                            #path=r\"C:\\Users\\dell\\n\" + name\n",
    "                            path=os.path.normpath('C:/Users/dell/'+name) \n",
    "                            #print(path)\n",
    "\n",
    "                           \n",
    "                            image_path=StoreonGoogleDrive(name,\"1SOgDeN-sZ6HFyDZxghUJuUq9M_O2vdUA\",path) #this function stores the image with that name from path on google drive\n",
    "                            #print(image_path)\n",
    "\n",
    "                            image = cv2.imread(name)\n",
    "                            features = cd.describe(image)\n",
    "                            feature_vectors.append([])\n",
    "                            feature_vectors[image_count-1]=features\n",
    "\n",
    "\n",
    "                            images_arr.append(image_path)\n",
    "                            image_count=image_count+1\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                        db_collection=getDbCon(Brand,Gender) #specify brand and gender to get db\n",
    "                        #store data in db\n",
    "                        id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "                        print(id1)\n",
    "                        image_count=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                product_count+=1\n",
    "                print(\"\")\n",
    "\n",
    "            page_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fb8b71fcec27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCambridgeScrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-4d675d4a3f14>\u001b[0m in \u001b[0;36mCambridgeScrape\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbrowser2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"window.scrollTo(0, document.body.scrollHeight);\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mmyElem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpresence_of_element_located\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'product-4939290738774'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Page is ready!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py\u001b[0m in \u001b[0;36muntil\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_find_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py\u001b[0m in \u001b[0;36m_find_element\u001b[1;34m(driver, by)\u001b[0m\n\u001b[0;32m    409\u001b[0m     if thrown.\"\"\"\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mNoSuchElementException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             'value': value})['value']\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     70\u001b[0m             return self.request_encode_body(method, url, fields=fields,\n\u001b[0;32m     71\u001b[0m                                             \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                                             **urlopen_kw)\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     def request_encode_url(self, method, url, fields=None, headers=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#scrape all brands one by one\n",
    "CambridgeScrape()\n",
    "JdotScrape()\n",
    "KhaadiScrape()\n",
    "OutfitterScrape()\n",
    "LimelightScrape()\n",
    "SapphireScrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
